{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "eb2b2466",
      "metadata": {},
      "source": [
        "# Sequence generation + embeddings (sequential model loading)\n",
        "\n",
        "This notebook fetches gene sequences, generates sequences per model,\n",
        "then creates cross-embeddings while loading one model at a time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "63859cd6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 \ubaa8\ub4e0 \ubaa8\ub4c8 \uc784\ud3ec\ud2b8 \uc644\ub8cc\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import gc\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "\n",
        "current_dir = Path('.').resolve()\n",
        "project_root = current_dir.parent\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "from preparation import get_device, iter_models, load_model\n",
        "from sequence_generation import (\n",
        "    fetch_gene_sequences,\n",
        "    sort_genes_by_length,\n",
        "    DEFAULT_DECODING_STRATEGIES,\n",
        ")\n",
        "\n",
        "print('\u2705 \ubaa8\ub4e0 \ubaa8\ub4c8 \uc784\ud3ec\ud2b8 \uc644\ub8cc')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "393dc17c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NCBI email: dlalswo0321@gmail.com\n",
            "ITERATIONS=30, MASK_RATIO=0.2\n",
            "\ubaa8\ub378: ['DNABERT-2', 'NT-v2-50m', 'NT-v2-500m']\n"
          ]
        }
      ],
      "source": [
        "NCBI_EMAIL = 'dlalswo0321@gmail.com'\n",
        "ITERATIONS = 30\n",
        "MASK_RATIO = 0.2\n",
        "\n",
        "RESULTS_DIR = Path('results')\n",
        "SEQ_DIR = RESULTS_DIR / 'sequences'\n",
        "EMB_DIR = RESULTS_DIR / 'embeddings'\n",
        "SEQ_DIR.mkdir(parents=True, exist_ok=True)\n",
        "EMB_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "model_configs = {\n",
        "    'DNABERT-2': 'zhihan1996/DNABERT-2-117M',\n",
        "    'NT-v2-50m': 'InstaDeepAI/nucleotide-transformer-v2-50m-multi-species',\n",
        "    'NT-v2-500m': 'InstaDeepAI/nucleotide-transformer-v2-500m-multi-species',\n",
        "}\n",
        "torch_dtype = 'auto'\n",
        "model_labels = list(model_configs.keys())\n",
        "\n",
        "print(f'NCBI email: {NCBI_EMAIL}')\n",
        "print(f'ITERATIONS={ITERATIONS}, MASK_RATIO={MASK_RATIO}')\n",
        "print(f'\ubaa8\ub378: {model_labels}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cf83f144",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch Version: 2.9.1\n",
            "Using device: mps\n",
            "\uc0ac\uc6a9 \uc911\uc778 \ub514\ubc14\uc774\uc2a4: mps\n",
            "Fetching gene sequences from NCBI...\n",
            "Fetching H4C1 by UID NM_003538...\n",
            "  \u2705 Found and added sequence for H4C1 (Length: 402bp)\n",
            "Fetching TP53 by UID NM_000546...\n",
            "  \u2705 Found and added sequence for TP53 (Length: 2512bp)\n",
            "Fetching HBB by UID NM_000518...\n",
            "  \u2705 Found and added sequence for HBB (Length: 628bp)\n",
            "Fetching HOXC11 by UID NM_014212...\n",
            "  \u2705 Found and added sequence for HOXC11 (Length: 3261bp)\n",
            "Fetching HOTAIR by UID NR_003716...\n",
            "  \u2705 Found and added sequence for HOTAIR (Length: 2273bp)\n",
            "Fetching VEGFA by UID NM_003376...\n",
            "  \u2705 Found and added sequence for VEGFA (Length: 3609bp)\n",
            "Fetching NEAT1 by UID NR_003513...\n",
            "  \u2705 Found and added sequence for NEAT1 (Length: 3190bp)\n",
            "Fetching NORAD by UID NR_027451...\n",
            "  \u2705 Found and added sequence for NORAD (Length: 5378bp)\n",
            "Fetching STAT3 by UID NM_139276...\n",
            "  \u2705 Found and added sequence for STAT3 (Length: 4921bp)\n",
            "Fetching PTEN by UID NM_000314...\n",
            "  \u2705 Found and added sequence for PTEN (Length: 8515bp)\n",
            "Fetching PTENP1 by UID NR_023917...\n",
            "  \u2705 Found and added sequence for PTENP1 (Length: 3932bp)\n",
            "Fetching GAPDH by UID NM_002046...\n",
            "  \u2705 Found and added sequence for GAPDH (Length: 1285bp)\n",
            "Fetching GAPDHP1 by UID NG_001123...\n",
            "  \u2705 Found and added sequence for GAPDHP1 (Length: 1098bp)\n",
            "Fetching TPI1 by UID NM_000365...\n",
            "  \u2705 Found and added sequence for TPI1 (Length: 1351bp)\n",
            "Fetching TPI1P1 by UID NG_008262.2...\n",
            "  \u2705 Found and added sequence for TPI1P1 (Length: 1471bp)\n",
            "Fetching GBA by UID NM_000157...\n",
            "  \u2705 Found and added sequence for GBA (Length: 2291bp)\n",
            "Fetching GBAP1 by UID NR_002777...\n",
            "  \u2705 Found and added sequence for GBAP1 (Length: 1277bp)\n",
            "Fetching STRC by UID NM_153700...\n",
            "  \u2705 Found and added sequence for STRC (Length: 5515bp)\n",
            "Fetching STRCP1 by UID NR_146078...\n",
            "  \u2705 Found and added sequence for STRCP1 (Length: 5415bp)\n",
            "\n",
            "\u2705 Gene fetching complete!\n",
            "Genes successfully loaded: ['H4C1', 'TP53', 'HBB', 'HOXC11', 'HOTAIR', 'VEGFA', 'NEAT1', 'NORAD', 'STAT3', 'PTEN', 'PTENP1', 'GAPDH', 'GAPDHP1', 'TPI1', 'TPI1P1', 'GBA', 'GBAP1', 'STRC', 'STRCP1']\n",
            "\uc218\uc9d1\ub41c \uc720\uc804\uc790:\n",
            "  - H4C1: 402 bp\n",
            "  - HBB: 628 bp\n",
            "  - GAPDHP1: 1098 bp\n",
            "  - GBAP1: 1277 bp\n",
            "  - GAPDH: 1285 bp\n",
            "  - TPI1: 1351 bp\n",
            "  - TPI1P1: 1471 bp\n",
            "  - HOTAIR: 2273 bp\n",
            "  - GBA: 2291 bp\n",
            "  - TP53: 2512 bp\n",
            "  - NEAT1: 3190 bp\n",
            "  - HOXC11: 3261 bp\n",
            "  - VEGFA: 3609 bp\n",
            "  - PTENP1: 3932 bp\n",
            "  - STAT3: 4921 bp\n",
            "  - NORAD: 5378 bp\n",
            "  - STRCP1: 5415 bp\n",
            "  - STRC: 5515 bp\n",
            "  - PTEN: 8515 bp\n"
          ]
        }
      ],
      "source": [
        "device = get_device()\n",
        "print(f'\uc0ac\uc6a9 \uc911\uc778 \ub514\ubc14\uc774\uc2a4: {device}')\n",
        "\n",
        "gene_selection = fetch_gene_sequences(NCBI_EMAIL)\n",
        "gene_selection = sort_genes_by_length(gene_selection)\n",
        "\n",
        "print('\uc218\uc9d1\ub41c \uc720\uc804\uc790:')\n",
        "for gene, seq in gene_selection.items():\n",
        "    print(f'  - {gene}: {len(seq)} bp')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "dc6f1c9d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\uc2dc\ud000\uc2a4 \uc0dd\uc131 \uc911...\n",
            "\ud83d\udce5 Downloading DNABERT-2...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "899f9f194f794c11bff96855bf7c42e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 DNABERT-2 Triton patch applied successfully.\n",
            "[DNABERT-2] Loading model...\n",
            "[DNABERT-2] Model loaded successfully.\n",
            "\u2705 DNABERT-2 loaded successfully.\n",
            "\ubaa8\ub378: DNABERT-2\n",
            "  H4C1..."
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/leeminjae/.cache/huggingface/modules/transformers_modules/_7bce263b15377fc15361f52cfab88f8b586abda0/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \u2713 (4 strategies)\n",
            "  HBB... \u2713 (4 strategies)\n",
            "  GAPDHP1... \u2713 (4 strategies)\n",
            "  GBAP1..."
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     strategy_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstrategy_base_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_t\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 27\u001b[0m generated_sequences \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moriginal_sequence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mITERATIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMASK_RATIO\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m results_data[strategy_key] \u001b[38;5;241m=\u001b[39m generated_sequences\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m generated_sequences\n",
            "File \u001b[0;32m~/Desktop/01_Projects/iterative reconstruction of DNA sequence/iterative-dna-reconstruction/preparation.py:295\u001b[0m, in \u001b[0;36mSequenceEvolver.run\u001b[0;34m(self, sequence, steps, mask_ratio, strategy, temperature, top_k, save_all, save_interval)\u001b[0m\n\u001b[1;32m    292\u001b[0m sequence_history \u001b[38;5;241m=\u001b[39m [current_seq] \u001b[38;5;28;01mif\u001b[39;00m save_all \u001b[38;5;129;01mor\u001b[39;00m save_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps):\n\u001b[0;32m--> 295\u001b[0m     current_seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevolve_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcurrent_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m save_all \u001b[38;5;129;01mor\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m save_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m step \u001b[38;5;241m==\u001b[39m steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    300\u001b[0m         sequence_history\u001b[38;5;241m.\u001b[39mappend(current_seq)\n",
            "File \u001b[0;32m~/Desktop/01_Projects/iterative reconstruction of DNA sequence/iterative-dna-reconstruction/preparation.py:247\u001b[0m, in \u001b[0;36mSequenceEvolver.evolve_step\u001b[0;34m(self, current_sequence, mask_ratio, strategy, temperature, top_k)\u001b[0m\n\u001b[1;32m    244\u001b[0m input_ids[\u001b[38;5;241m0\u001b[39m, mask_indices] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mmask_token_id\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 247\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m    250\u001b[0m mask_logits \u001b[38;5;241m=\u001b[39m logits[\u001b[38;5;241m0\u001b[39m, mask_indices, :]\n",
            "File \u001b[0;32m/opt/homebrew/Cellar/micromamba/envs/dna-fm/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/Cellar/micromamba/envs/dna-fm/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/_7bce263b15377fc15361f52cfab88f8b586abda0/bert_layers.py:744\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    740\u001b[0m     masked_tokens_mask \u001b[38;5;241m=\u001b[39m labels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    742\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 744\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmasked_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmasked_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    759\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    760\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls(sequence_output)\n",
            "File \u001b[0;32m/opt/homebrew/Cellar/micromamba/envs/dna-fm/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/Cellar/micromamba/envs/dna-fm/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/_7bce263b15377fc15361f52cfab88f8b586abda0/bert_layers.py:596\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, position_ids, output_all_encoded_layers, masked_tokens_mask, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token_type_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    594\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(input_ids)\n\u001b[0;32m--> 596\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m subset_mask \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    600\u001b[0m first_col_mask \u001b[38;5;241m=\u001b[39m []\n",
            "File \u001b[0;32m/opt/homebrew/Cellar/micromamba/envs/dna-fm/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/Cellar/micromamba/envs/dna-fm/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/_7bce263b15377fc15361f52cfab88f8b586abda0/bert_layers.py:96\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m     91\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     92\u001b[0m                                      dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong,\n\u001b[1;32m     93\u001b[0m                                      device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embeddings\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;66;03m# type: ignore  # yapf: disable\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m token_type_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[1;32m     99\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m token_type_embeddings\n",
            "File \u001b[0;32m/opt/homebrew/Cellar/micromamba/envs/dna-fm/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/Cellar/micromamba/envs/dna-fm/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/opt/homebrew/Cellar/micromamba/envs/dna-fm/lib/python3.10/site-packages/torch/nn/modules/sparse.py:192\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/Cellar/micromamba/envs/dna-fm/lib/python3.10/site-packages/torch/nn/functional.py:2542\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2536\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2537\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2540\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2541\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2542\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print('\uc2dc\ud000\uc2a4 \uc0dd\uc131 \uc911...')\n",
        "for model_label, model_instance in iter_models(\n",
        "    device, model_configs, torch_dtype=torch_dtype\n",
        "):\n",
        "    print(f'\ubaa8\ub378: {model_label}')\n",
        "\n",
        "    model_name = model_label.replace('/', '-')\n",
        "    model_dir = SEQ_DIR / model_name\n",
        "    model_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    gene_iter = tqdm(\n",
        "        gene_selection.items(),\n",
        "        desc=f'{model_label} genes',\n",
        "        leave=False,\n",
        "    )\n",
        "\n",
        "    for gene_id, original_sequence in gene_iter:\n",
        "        output_csv = model_dir / f'{gene_id}.csv'\n",
        "        results_data = {}\n",
        "\n",
        "        for strategy_base_key, strategy_cfg in DEFAULT_DECODING_STRATEGIES.items():\n",
        "            strategy_type = strategy_cfg['type']\n",
        "            temperatures = strategy_cfg.get('temperatures', [1.0])\n",
        "            top_k = strategy_cfg.get('top_k', 50)\n",
        "\n",
        "            for temp in temperatures:\n",
        "                if strategy_type == 'greedy':\n",
        "                    strategy_key = strategy_base_key\n",
        "                else:\n",
        "                    strategy_key = f'{strategy_base_key}_t{temp}'\n",
        "\n",
        "                generated_sequences = model_instance.run(\n",
        "                    sequence=original_sequence,\n",
        "                    steps=ITERATIONS,\n",
        "                    mask_ratio=MASK_RATIO,\n",
        "                    strategy=strategy_type,\n",
        "                    temperature=temp,\n",
        "                    top_k=top_k,\n",
        "                    save_all=True,\n",
        "                    save_interval=1,\n",
        "                )\n",
        "\n",
        "                results_data[strategy_key] = generated_sequences\n",
        "\n",
        "                del generated_sequences\n",
        "                gc.collect()\n",
        "                if device == 'cuda':\n",
        "                    torch.cuda.empty_cache()\n",
        "                elif device == 'mps':\n",
        "                    torch.mps.empty_cache()\n",
        "\n",
        "        df = pd.DataFrame(results_data).T\n",
        "        df.columns = [f'iteration_{i}' for i in range(df.shape[1])]\n",
        "        df.to_csv(output_csv)\n",
        "\n",
        "    del model_instance\n",
        "    gc.collect()\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "    elif device == 'mps':\n",
        "        torch.mps.empty_cache()\n",
        "\n",
        "print('\u2705 \uc2dc\ud000\uc2a4 \uc0dd\uc131 \uc644\ub8cc')\n",
        "\n",
        "removed = 0\n",
        "for cache_file in EMB_DIR.glob('**/*.pkl'):\n",
        "    try:\n",
        "        cache_file.unlink()\n",
        "        removed += 1\n",
        "    except Exception as e:\n",
        "        print(f'  \uce90\uc2dc \uc0ad\uc81c \uc2e4\ud328: {cache_file} ({e})')\n",
        "print(f'\ud83e\uddf9 \uae30\uc874 \uc784\ubca0\ub529 \uce90\uc2dc \uc0ad\uc81c \uc644\ub8cc: {removed} files')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5257554",
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ubaa8\ub4e0 \uc2dc\ud000\uc2a4 \ub85c\ub4dc\n",
        "all_sequences = {}\n",
        "\n",
        "for model_key in model_labels:\n",
        "    model_name = model_key.replace('/', '-')\n",
        "    model_dir = SEQ_DIR / model_name\n",
        "    all_sequences[model_name] = {}\n",
        "\n",
        "    for csv_file in model_dir.glob('*.csv'):\n",
        "        gene_id = csv_file.stem\n",
        "        df = pd.read_csv(csv_file, index_col=0)\n",
        "        sequences_dict = {}\n",
        "        for strategy in df.index:\n",
        "            sequences_dict[strategy] = df.loc[strategy].tolist()\n",
        "        all_sequences[model_name][gene_id] = sequences_dict\n",
        "\n",
        "print('\u2705 \uc2dc\ud000\uc2a4 \ub85c\ub4dc \uc644\ub8cc')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8548b1c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_cross_embeddings(source_sequences, target_model_instance):\n",
        "    cross_emb = {}\n",
        "    for gene_id, strategies in source_sequences.items():\n",
        "        cross_emb[gene_id] = {}\n",
        "        for strategy, sequences in strategies.items():\n",
        "            if not sequences:\n",
        "                continue\n",
        "            embeddings = []\n",
        "            for seq in sequences:\n",
        "                if pd.isna(seq) or seq == '':\n",
        "                    continue\n",
        "                emb = target_model_instance.get_embedding(str(seq))\n",
        "                embeddings.append(emb)\n",
        "            if embeddings:\n",
        "                cross_emb[gene_id][strategy] = embeddings\n",
        "            gc.collect()\n",
        "            if device == 'cuda':\n",
        "                torch.cuda.empty_cache()\n",
        "            elif device == 'mps':\n",
        "                torch.mps.empty_cache()\n",
        "    return cross_emb\n",
        "\n",
        "print('\uc784\ubca0\ub529 \uc0dd\uc131 \uc911...')\n",
        "generator_labels = list(model_labels)\n",
        "evaluator_labels = list(model_labels)\n",
        "\n",
        "cross_embeddings = {}\n",
        "\n",
        "for eval_label in evaluator_labels:\n",
        "    model_instance = load_model(\n",
        "        device,\n",
        "        eval_label,\n",
        "        model_configs[eval_label],\n",
        "        torch_dtype=torch_dtype,\n",
        "    )\n",
        "\n",
        "    for gen_label in generator_labels:\n",
        "        cache_name = f'embeddings_{gen_label}__by__{eval_label}.pkl'\n",
        "        cache_path = EMB_DIR / cache_name\n",
        "\n",
        "        print(f'\uc784\ubca0\ub529 \uc0dd\uc131: {gen_label} sequences \u2192 {eval_label} evaluator')\n",
        "        cross_emb = build_cross_embeddings(\n",
        "            all_sequences[gen_label.replace('/', '-')],\n",
        "            model_instance,\n",
        "        )\n",
        "        cross_embeddings[(gen_label, eval_label)] = cross_emb\n",
        "        with open(cache_path, 'wb') as f:\n",
        "            import pickle\n",
        "            pickle.dump(cross_emb, f, protocol=4)\n",
        "        print(f'  \u2713 \uc800\uc7a5: {cache_path}')\n",
        "\n",
        "    del model_instance\n",
        "    gc.collect()\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "    elif device == 'mps':\n",
        "        torch.mps.empty_cache()\n",
        "\n",
        "print('\u2705 \uc784\ubca0\ub529 \uc0dd\uc131 \uc644\ub8cc')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dna-fm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}