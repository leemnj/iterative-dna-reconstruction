{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2709272c",
   "metadata": {},
   "source": [
    "# Model info: DNABERT-2 vs NT-v2-500m\n",
    "\n",
    "This notebook loads **tokenizers and configs only** (no model weights) to avoid kernel crashes, then prints tokenizer/model metadata and the stride/window math used in `SequenceEvolver.evolve_step` (see `01_generate-seq-and-emb/preparation.py`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef7e586f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DNABERT-2', 'NT-v2-500m']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig\n",
    "\n",
    "MODEL_CONFIGS = {\n",
    "    'DNABERT-2': 'zhihan1996/DNABERT-2-117M',\n",
    "    'NT-v2-500m': 'InstaDeepAI/nucleotide-transformer-v2-500m-multi-species',\n",
    "}\n",
    "\n",
    "tokenizers = {}\n",
    "configs = {}\n",
    "for label, model_id in MODEL_CONFIGS.items():\n",
    "    tokenizers[label] = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    configs[label] = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "list(tokenizers.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afa6d03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>model_id_or_path</th>\n",
       "      <th>tokenizer_class</th>\n",
       "      <th>vocab_size</th>\n",
       "      <th>model_max_length(tokenizer)</th>\n",
       "      <th>num_special_tokens_to_add</th>\n",
       "      <th>special_tokens</th>\n",
       "      <th>evolve_max_length</th>\n",
       "      <th>evolve_window_size</th>\n",
       "      <th>evolve_stride</th>\n",
       "      <th>config.max_position_embeddings</th>\n",
       "      <th>config.n_positions</th>\n",
       "      <th>config.model_type</th>\n",
       "      <th>config.hidden_size</th>\n",
       "      <th>config.num_attention_heads</th>\n",
       "      <th>config.num_hidden_layers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNABERT-2</td>\n",
       "      <td>zhihan1996/DNABERT-2-117M</td>\n",
       "      <td>PreTrainedTokenizerFast</td>\n",
       "      <td>4096</td>\n",
       "      <td>1000000000000000019884624838656</td>\n",
       "      <td>2</td>\n",
       "      <td>{'cls': '[CLS]', 'sep': '[SEP]', 'pad': '[PAD]', 'mask': '[MASK]', 'unk': '[UNK]'}</td>\n",
       "      <td>1024</td>\n",
       "      <td>1022</td>\n",
       "      <td>511</td>\n",
       "      <td>512</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>768</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NT-v2-500m</td>\n",
       "      <td>InstaDeepAI/nucleotide-transformer-v2-500m-multi-species</td>\n",
       "      <td>EsmTokenizer</td>\n",
       "      <td>4107</td>\n",
       "      <td>2048</td>\n",
       "      <td>1</td>\n",
       "      <td>{'cls': '&lt;cls&gt;', 'sep': None, 'pad': '&lt;pad&gt;', 'mask': '&lt;mask&gt;', 'unk': '&lt;unk&gt;'}</td>\n",
       "      <td>1024</td>\n",
       "      <td>1023</td>\n",
       "      <td>511</td>\n",
       "      <td>2050</td>\n",
       "      <td>None</td>\n",
       "      <td>esm</td>\n",
       "      <td>1024</td>\n",
       "      <td>16</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                          model_id_or_path  \\\n",
       "0   DNABERT-2                                 zhihan1996/DNABERT-2-117M   \n",
       "1  NT-v2-500m  InstaDeepAI/nucleotide-transformer-v2-500m-multi-species   \n",
       "\n",
       "           tokenizer_class  vocab_size      model_max_length(tokenizer)  \\\n",
       "0  PreTrainedTokenizerFast        4096  1000000000000000019884624838656   \n",
       "1             EsmTokenizer        4107                             2048   \n",
       "\n",
       "   num_special_tokens_to_add  \\\n",
       "0                          2   \n",
       "1                          1   \n",
       "\n",
       "                                                                       special_tokens  \\\n",
       "0  {'cls': '[CLS]', 'sep': '[SEP]', 'pad': '[PAD]', 'mask': '[MASK]', 'unk': '[UNK]'}   \n",
       "1     {'cls': '<cls>', 'sep': None, 'pad': '<pad>', 'mask': '<mask>', 'unk': '<unk>'}   \n",
       "\n",
       "   evolve_max_length  evolve_window_size  evolve_stride  \\\n",
       "0               1024                1022            511   \n",
       "1               1024                1023            511   \n",
       "\n",
       "   config.max_position_embeddings config.n_positions config.model_type  \\\n",
       "0                             512               None                     \n",
       "1                            2050               None               esm   \n",
       "\n",
       "   config.hidden_size  config.num_attention_heads  config.num_hidden_layers  \n",
       "0                 768                          12                        12  \n",
       "1                1024                          16                        29  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "\n",
    "def summarize(label, tok, cfg):\n",
    "    # Stride/window math used in SequenceEvolver.evolve_step\n",
    "    # NOTE: evolve_step hardcodes max_length=1024 in preparation.py\n",
    "    max_len = 1024\n",
    "    num_special = tok.num_special_tokens_to_add(pair=False)\n",
    "    window_size = max(1, max_len - num_special)\n",
    "    stride = max(1, window_size // 2)\n",
    "\n",
    "    def get_cfg(key, default=None):\n",
    "        return getattr(cfg, key, default)\n",
    "\n",
    "    return {\n",
    "        'label': label,\n",
    "        'model_id_or_path': tok.name_or_path,\n",
    "        'tokenizer_class': tok.__class__.__name__,\n",
    "        'vocab_size': tok.vocab_size,\n",
    "        'model_max_length(tokenizer)': tok.model_max_length,\n",
    "        'num_special_tokens_to_add': num_special,\n",
    "        'special_tokens': {\n",
    "            'cls': tok.cls_token,\n",
    "            'sep': tok.sep_token,\n",
    "            'pad': tok.pad_token,\n",
    "            'mask': tok.mask_token,\n",
    "            'unk': tok.unk_token,\n",
    "        },\n",
    "        'evolve_max_length': max_len,\n",
    "        'evolve_window_size': window_size,\n",
    "        'evolve_stride': stride,\n",
    "        'config.max_position_embeddings': get_cfg('max_position_embeddings'),\n",
    "        'config.n_positions': get_cfg('n_positions'),\n",
    "        'config.model_type': get_cfg('model_type'),\n",
    "        'config.hidden_size': get_cfg('hidden_size'),\n",
    "        'config.num_attention_heads': get_cfg('num_attention_heads'),\n",
    "        'config.num_hidden_layers': get_cfg('num_hidden_layers'),\n",
    "    }\n",
    "\n",
    "rows = [summarize(label, tokenizers[label], configs[label]) for label in tokenizers]\n",
    "df = pd.DataFrame(rows)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7f3fee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unk_token': '<unk>',\n",
       " 'pad_token': '<pad>',\n",
       " 'cls_token': '<cls>',\n",
       " 'mask_token': '<mask>'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizers['NT-v2-500m'].special_tokens_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a764fd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'unk_token': '<unk>',\n",
       " 'pad_token': '<pad>',\n",
       " 'cls_token': '<cls>',\n",
       " 'mask_token': '<mask>'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = tokenizers[\"NT-v2-500m\"]\n",
    "\n",
    "# 전체 vocab 크기\n",
    "print(tok.vocab_size)\n",
    "\n",
    "# 실제 토큰 문자열 확인 (앞쪽 50개)\n",
    "vocab = tok.get_vocab()\n",
    "sorted_vocab = sorted(vocab.items(), key=lambda x: x[1])\n",
    "sorted_vocab[:50]\n",
    "\n",
    "# special tokens 확인\n",
    "tok.special_tokens_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcf810c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GGGTAG',\n",
       " 'GGGTTA',\n",
       " 'GGGTTT',\n",
       " 'GGGTTC',\n",
       " 'GGGTTG',\n",
       " 'GGGTCA',\n",
       " 'GGGTCT',\n",
       " 'GGGTCC',\n",
       " 'GGGTCG',\n",
       " 'GGGTGA',\n",
       " 'GGGTGT',\n",
       " 'GGGTGC',\n",
       " 'GGGTGG',\n",
       " 'GGGCAA',\n",
       " 'GGGCAT',\n",
       " 'GGGCAC',\n",
       " 'GGGCAG',\n",
       " 'GGGCTA',\n",
       " 'GGGCTT',\n",
       " 'GGGCTC',\n",
       " 'GGGCTG',\n",
       " 'GGGCCA',\n",
       " 'GGGCCT',\n",
       " 'GGGCCC',\n",
       " 'GGGCCG',\n",
       " 'GGGCGA',\n",
       " 'GGGCGT',\n",
       " 'GGGCGC',\n",
       " 'GGGCGG',\n",
       " 'GGGGAA',\n",
       " 'GGGGAT',\n",
       " 'GGGGAC',\n",
       " 'GGGGAG',\n",
       " 'GGGGTA',\n",
       " 'GGGGTT',\n",
       " 'GGGGTC',\n",
       " 'GGGGTG',\n",
       " 'GGGGCA',\n",
       " 'GGGGCT',\n",
       " 'GGGGCC',\n",
       " 'GGGGCG',\n",
       " 'GGGGGA',\n",
       " 'GGGGGT',\n",
       " 'GGGGGC',\n",
       " 'GGGGGG',\n",
       " 'A',\n",
       " 'T',\n",
       " 'C',\n",
       " 'G',\n",
       " 'N']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# special tokens id 들\n",
    "tok.special_tokens_map, tok.all_special_tokens, tok.all_special_ids\n",
    "\n",
    "# special 제외한 실제 k-mer만 보고 싶으면\n",
    "special = set(tok.all_special_tokens)\n",
    "kmer_only = [t for t,_ in sorted_vocab if t not in special]\n",
    "len(kmer_only)\n",
    "kmer_only[-20:]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dna-fm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
